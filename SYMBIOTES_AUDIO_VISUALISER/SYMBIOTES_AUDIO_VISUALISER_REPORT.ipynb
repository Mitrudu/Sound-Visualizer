{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "PROJECT TITLE: AUDIO VISUALISER BY SYMBIOTES\n\nGITHUB LINK FOR PROJECT FILES: https://github.com/Mitrudu/Sound-Visualizer\n\nTEAM MEMBERS:\nGoli Aananda Vardhan\t-\t180101026\nAluri Aravind Lenin\t\t-\t190101009\nPathlavath Srikanth\t\t-\t190101060\nUday Singh\t\t\t\t-\t190101095\nVanja Vivek Vardhan\t\t-\t190101097\nDesaboina Hemakrishna\t-\t190102023\n\nOBJECTIVE:\nThe objective of this project is to analyse audio signals and extract frequency & amplitude and visually represent the audio so that the viewer's experience of the audio can be enhanced by viewing at the visual.\n\nBACKGROUND:\nAn audio signal or sound signal has many features, such as frequency, intensity, noise, etc. To study the signal more effectively, we are trying to visualize the signal. In investment banking, some traders analyse the performance of a particular stock by converting its stock value (in graph) to audio signals and listening to them to work efficiently. We are similarly converting an audio signal into a visual signal.\n\nMETHODOLOGY:\nWe will take an audio and we will extract the frequencies and their corresponding amplitudes. We will use fourier transform on the audio signal to get the frequencies and amplitudes. And we will map different frequencies of the audio with different colors so that the viewer distinguish the sounds. Depending upon the amplitudes of some frequencies, we'll represnt some bubbles/circles on the screen along with other visualisations as well.\n\nCODE AND EXPLANATION:\nWe have mostly used javaScript and Web Audio API and canvas 2d graphics methods to visalise the audio.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n\t<meta charset=\"UTF-8\">\n\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\t<title>Page Title</title>\n    <link rel=\"stylesheet\" href=\"final_style.css\">\n</head>\n<body>\n\t\n   <div id ='container'>\n        <canvas id=\"canvas1\"></canvas>\n        <audio id=\"audio1\" controls></audio>\n        <input type='file' id='fileupload' accept='audio/*' />\n        <canvas id=\"visualizer\"></canvas>\n        <button id=\"recordBtn\">Record</button>\n    </div>\n    \n    <script src=\"final_script.js\"></script>\n    \n</body>\n</html>\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The above is the basic HTML code for the webpage. It add's a \"choose file\" button, \"record\" button and audio controls.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "file.addEventListener(\"change\", function () {\n  console.log(this.files);\n  const files = this.files;\n  const audio1 = document.getElementById(\"audio1\");\n  audio1.src = URL.createObjectURL(files[0]);\n  audio1.load();",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We added event listeners to trigger the visualisations. This part of the code snippet does this: When we choose a audio file from the menu, the code makes a URL path for that and makes it as source for the audio to play.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We took the help of web Audio API for javascript. And we called the API into our file by using this command:",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "audioCtx = new AudioContext();",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Through this audioCtx variable, we can able to access all the functionalties of Web Audio API.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "audioSource = audioCtx.createMediaElementSource(audio1);\nanalyser = audioCtx.createAnalyser();\naudioSource.connect(analyser);\nanalyser.connect(audioCtx.destination);",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The \"audioSource = audioCtx.createMediaElementSource(audio1);\" creates a audio source node and that source is the audio file that we have selected.\nThe \"analyser = audioCtx.createAnalyser();\" creates a analyser node. This analyser node is used to analyze the audio data from the MediaElementSource and extract frequency and time-domain data.\nThe \"audioSource.connect(analyser);\" connects source and analyser. This allows the audio data from the audioSource to be processed by the analyser.\nThe \"analyser.connect(audioCtx.destination);\" outputs the audio through the destination, by default, it's our system speakers.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "navigator.mediaDevices.getUserMedia({ audio: true })\n    .then(function(stream) {",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This code uses the browser's built-in capabilities to capture audio from a user's device, like a microphone or headphones. It uses a method called getUserMedia() to access the user's audio device. It will ask the user throgh prompt that \"ALLOW TO USE MICROPHONE\". After the user gives the permission, the live audio will be set as source node and it will be connected to the analyser node.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "analyser.fftSize = 2048;\n\n  const bufferLength = analyser.frequencyBinCount;\n  const dataArray = new Uint8Array(bufferLength);",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The fftSize property of the analyser node is set to 2048, which determines the number of data points used in the Fast Fourier Transform (FFT) algorithm to analyze the audio data. The code then creates a new Uint8Array object called dataArray with a length equal to the frequencyBinCount property of the analyser node. This array will be used to store the frequency data generated by the analyser node.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "And for the graphics part, we have used/called canvas 2d graphics methods by using this command:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "const ctx = canvas.getContext(\"2d\");",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Through this \"ctx\" variable, we will able to access all the canvas 2d methods in our program.\nAnd the visualisation that we did, consist of 3 parts: the bar viz + the side curve lines viz + the popping bubble viz. (viz = visualisation). And we have coded for these 3 parts. And here is the code for \"the bar viz\" part:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "function drawVisualiser1(bufferLength, x, barWidth, barHeight, dataArray) {\n  for (let i = 0; i < bufferLength; i++) {\n    barHeight = dataArray[i];\n    const red = i * barHeight / 10;\n    const blue = i * barWidth+10;\n    const green = barHeight / 3;\n    ctx.fillStyle = 'rgb(' + red + ',' + blue + ',' + green + ')';\n    ctx.fillRect(canvas.width/2 - x, canvas.height - barHeight, barWidth, barHeight);\n    x += barWidth;\n  }\n    \n  for (let i = 0; i < bufferLength; i++) {\n    barHeight = dataArray[i];\n    const red = i * barHeight / 10;\n    const blue = i * barWidth+10;\n    const green = barHeight / 3;\n    ctx.fillStyle = 'rgb(' + red + ',' + blue + ',' + green + ')';\n    ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);\n    x += barWidth;\n  }\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The function loops through the dataArray and draws bars on the canvas based on the values in dataArray. Each bar's height and color are determined by its position in the loop.\n\nThe first loop starts from the midde of the canvas, drawing bars from left to right, while the second loop draws bars from right to left.\n\nThe barWidth, barHeight, and x values are used to determine the dimensions and position of each bar. The red, blue, and green values are used to determine the color of each bar, which is set using the ctx.fillStyle property and the fillRect() method.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "And the code for the \"the side curve line viz\" part is:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "function drawVisualiser2(bufferLength, x, barWidth, barHeight, dataArray) {\n  for (let i = 0; i < bufferLength; i++) {\n    barHeight = dataArray[i];\n    ctx.save();\n    ctx.translate(canvas.width/2, canvas.height/2);\n    ctx.rotate(i+Math.PI*2/bufferLength);\n    const red = i * barHeight / 10;\n    const blue = i * barWidth+10;\n    const green = barHeight / 3;\n    ctx.fillStyle = 'rgb(' + red + ',' + blue + ',' + green + ')';\n    ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);\n    x += barWidth;\n    ctx.restore();\n  }\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Similar to drawVisualiser1, this function loops through the dataArray and draws bars on a canvas based on the values in dataArray. But, instead of drawing bars in a straight line, the function rotates each bar around the center of the canvas.\n\nTo do this, the function uses the ctx.save() and ctx.restore() methods to save and restore the state of the canvas context. Within each iteration of the loop, the canvas context is translated to the center of the canvas using ctx.translate(), rotated around the center using ctx.rotate(), and then restored to its original state.\n\nAnd the code for the \"popping bubbles viz\" is:",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "function drawVisualiser3(bufferLength, x, barWidth, barHeight, dataArray) {\n  // draw the circular line\n  ctx.beginPath();\n  ctx.arc(canvas.width / 2, canvas.height / 2, canvas.width / 10, 0, 2 * Math.PI);\n  ctx.lineWidth = 3;\n  ctx.strokeStyle = 'rgba(255, 255, 255, 0.5)';\n  ctx.stroke();\n\n  for (let i = 0; i < bufferLength; i++) {\n    barHeight = dataArray[i];\n    if (barHeight >= 200) {\n      // generate random bubble coordinates within the circle\n      const angle = Math.random() * Math.PI * 2;\n      const radius = (canvas.width / 10) * Math.sqrt(Math.random());\n      const bubbleX = canvas.width / 2 + radius * Math.cos(angle);\n      const bubbleY = canvas.height / 2 + radius * Math.sin(angle);\n\n      // draw the bubble\n      ctx.beginPath();\n      ctx.arc(bubbleX, bubbleY, 10, 0, 2 * Math.PI);\n      ctx.fillStyle = 'rgba(255, 255, 255, 0.5)';\n      ctx.fill();\n    }\n  }\n}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The drawVisualiser3 function loops through the dataArray, generating random bubble coordinates within the circular area if the barHeight is greater than or equal to 200. We draw the circle in the middle of the screen by using the 2d canvas methods that we mentioned previously. We can access all the canvas 2d methods throgh \"ctx\". Here, we are using: ctx.arc(canvas.width / 2, canvas.height / 2, canvas.width / 10, 0, 2 * Math.PI); to draw the circle.\n\nAnd we are using the same method that we used to draw the circle, to draw the bubbles as well, but here insted of a line stroke, we are filling the entire circle arc.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "HOW DOES THIS PROJECT RELATES TO THIS COURSE?\nThis project involved extracting frequencies from the audio signals using fast fourier transform which was taught in the course. The fourier transform was very helpful also, as it converted signal from time domain to frequency domain, which helped us.",
      "metadata": {}
    }
  ]
}